
# –£—Ä–æ–∫ 24: –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤ (–æ—Å–Ω–æ–≤—ã)

# üî∏ –ß—Ç–æ —Ç–∞–∫–æ–µ pip –∏ –∑–∞—á–µ–º –æ–Ω –Ω—É–∂–µ–Ω

# pip ‚Äî —ç—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–∫–µ—Ç–æ–≤ –≤ Python.
# –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫, —Ç–∞–∫–∏—Ö –∫–∞–∫ requests –∏–ª–∏ beautifulsoup4,
# –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –≤—Ö–æ–¥—è—Ç –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É —è–∑—ã–∫–∞.

# –ü—Ä–∏–º–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —á–µ—Ä–µ–∑ —Ç–µ—Ä–º–∏–Ω–∞–ª:
# pip install requests
# pip install beautifulsoup4

# –≠—Ç–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω—É–∂–Ω—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü ‚Äî —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ —Ä–∞–∑–±–æ—Ä–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å–∞–π—Ç–∞.

# üî∏ –ß—Ç–æ —Ç–∞–∫–æ–µ –ø–∞—Ä—Å–∏–Ω–≥ (–≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥)

# –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤ ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å HTML-—Å—Ç—Ä–∞–Ω–∏—Ü.
# –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø—Ä–æ–µ–∫—Ç–∞—Ö –¥–ª—è —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π, —Ü–µ–Ω —Ç–æ–≤–∞—Ä–æ–≤, –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø–æ–≥–æ–¥—ã –∏ –¥—Ä—É–≥–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

# üî∏ –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:
# - requests ‚Äî –ø–æ–ª—É—á–∞–µ—Ç HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
# - BeautifulSoup ‚Äî —Ä–∞–∑–±–∏—Ä–∞–µ—Ç HTML –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å –Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã.

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞:
# pip install requests beautifulsoup4

# üî∏ –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä –∫–æ–¥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞

import requests
from bs4 import BeautifulSoup
#
# def basic_scraper():
#     url = "https://example.com"
#     try:
#         response = requests.get(url)
#         if response.status_code == 200:
#             soup = BeautifulSoup(response.text, "html.parser")
#             titles = soup.find_all("h1")
#             for t in titles:
#                 print(t.text)
#         else:
#             print("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:", response.status_code)
#     except Exception as e:
#         print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ:", e)
# basic_scraper()

# üî∏ –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã BeautifulSoup
# find()      ‚Äî –Ω–∞—Ö–æ–¥–∏—Ç –ø–µ—Ä–≤—ã–π —Ç–µ–≥ –ø–æ —É—Å–ª–æ–≤–∏—é
# find_all()  ‚Äî –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Ç–µ–≥–∏ –ø–æ —É—Å–ª–æ–≤–∏—é
# .text       ‚Äî –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Ç–µ–≥–∞
# [attr]      ‚Äî –ø–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, href, src)

# –ü—Ä–∏–º–µ—Ä—ã:
# soup.find("p", class_="info") (_ is important)
# soup.find("a")["href"]

# üî∏ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã

# def extract_links(url):
#     try:
#         response = requests.get(url)
#         soup = BeautifulSoup(response.text, "html.parser")
#         links = soup.find_all("a")
#         return [link.get("href") for link in links if link.get("href")]
#     except Exception as e:
#         return [f"–û—à–∏–±–∫–∞: {e}"]
#
# print(extract_links("https://example.com"))

# üî∏ –ü–∞—Ä—Å–∏–Ω–≥ —Ç–∞–±–ª–∏—Ü—ã

# def extract_table_rows(url):
#     try:
#         response = requests.get(url)
#         soup = BeautifulSoup(response.text, "html.parser")
#         rows = soup.find_all("a", class_="lf-ad-tile__link")
#         for row in rows:
#             print(row.text)
#         return None
#
#     except Exception as e:
#         return [f"–û—à–∏–±–∫–∞: {e}"]
#
# print(extract_table_rows("https://lalafo.kg/"))

# üî∏ –≠—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞
# - –£–≤–∞–∂–∞–π robots.txt —Å–∞–π—Ç–∞ (–æ–Ω –º–æ–∂–µ—Ç –∑–∞–ø—Ä–µ—â–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥)
# - –ò—Å–ø–æ–ª—å–∑—É–π –ø–∞—É–∑—ã –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (time.sleep)
# - –ù–µ —Å–æ–∑–¥–∞–≤–∞–π –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ —Å–µ—Ä–≤–µ—Ä
# - –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π, –º–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

# –î–ª—è –∑–∞–ø—É—Å–∫–∞:
# basic_scraper()
# print(extract_links("https://example.com"))